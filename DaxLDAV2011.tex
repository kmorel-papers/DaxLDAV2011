% -*- latex -*-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This beginning part of the preamble is specific to the vgtc document class.

\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1 
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{A Proposed Framework for Data Analysis and Visualization at Extreme Scale}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} %
%%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}}
%%\affiliation{\scriptsize Martha Stewart Enterprises \\ Microsoft Research}

%% Author and Affiliation (multiple authors with multiple affiliations)
\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
        \scriptsize Starbucks Research %
\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com}\\ %
     \scriptsize Grimley Widgets, Inc. %
\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}\\ %
     \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
%\teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
%}

%% Abstract section
\abstract{ Experts agree that the exascale machine will comprise processors
  that contain many cores, which in turn will necessitate a much higher
  degree of concurrency.  Software will require a minimum of a 1000 times
  more concurrency.  Most parallel analysis and visualization algorithms
  today work by partitioning data and running mostly serial algorithms
  concurrently on each data partition.  Although this approach lends itself
  well to the concurrency of current high performance computing, it does
  not exhibit the appropriate pervasive parallelism required for exascale
  computing. The data partitions are too small and the overhead of the
  threads too large to make effective use of all the cores in an extreme
  scale machine.  This paper introduces a new visualization framework
  designed to exhibit the pervasive parallelism necessary for extreme scale
  machines.  We demonstrate the use of this system on a GPU processor,
  which we feel is the best analog to an exascale node that we have
  available today.  }

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{
  \CCScat{D.1.3}{Software}{Programming Techniques}{Concurrent Programming}
}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

% End of vgtc-specific portion of the preamble.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{varioref}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage{hyperref}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{red}{rgb}{1,0,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\sticky}[1]{{\color{red}\textsc{[#1]}}}

\begin{document}

%% VGTC-specific:
%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle

%% \section{Introduction} 
\label{sec:Introduction}

Most of today's visualization libraries and applications are based off of
what is known as the \keyterm{visualization
  pipeline}\lcite{Haeberli88,Lucas92}.  The visualization pipeline is the
key metaphor in many visualization development systems such as the
Visualization Toolkit (VTK)\lcite{VTKBook}, SCIRun\lcite{SCIRunPaper}, the
Application Visualization System (AVS)\lcite{AVSPaper},
OpenDX\lcite{OpenDXPaper}, and Iris Explorer\lcite{IRISExplorerPaper}.  It
is also the internal mechanism or external interface for may end-user
visualization applications such as ParaView\lcite{ParaViewGuideBook},
VisIt\lcite{VisItBook}, VisTrails\lcite{VisTrailsPaper},
MayaVi\lcite{MayaViPaper}, VolView\lcite{VolViewBook},
OsiriX\lcite{OsiriXPaper}, 3D Slicer\lcite{3DSlicerPaper}, and
BioImageXD\lcite{BioImageXDPaper}.

In the visualization pipeline model, algorithms are encapsulated as
\keyterm{filter} components with inputs and outputs.  These filters can
be combined by connecting the outputs of one filter to the inputs of
another filter.  The visualization pipeline model is popular because it
provides a convenient abstraction that allows users to combine algorithms
in powerful ways.

Although the visualization pipeline lends itself well to the concurrency of
current high performance
computing\lcite{Moreland08,Patchett09,Pugmire08,White05}, its structure
prohibits the necessary extreme concurrency required for exascale
computers.  This paper describes the design of the \keyterm{Dax toolkit} to
perform Data Analysis at Extreme scales.  The computational unit of this
framework is a \keyterm{functor}\sticky{Need to decide on nomenclature and
  do search/replace.}, a single operation on a small piece of data.
Functors can be combined in much the same way as filters, but their light
weight, lack of state, and small data access make them more suitable for
the massive concurrency required by exascale computers and associated
multi- and many-core processors.

\section{Related Work}
\label{sec:RelatedWork}

Since its inception, many improvements have been made to the visualization
pipeline to allow it to function well with large data.  By dividing tasks,
pipelines, or data amongst processes, the serial algorithms of a
visualization pipeline can work in parallel with little or no
modification\lcite{Ahrens00}.  In particular, the data parallel
mode, partitioning the data and replicating the pipeline, performs
well on current high performance computers\lcite{Cedilnik06}.

The basic function of a visualization pipeline is to process data flowing
from upstream to downstream.  More recent visualization pipeline
implementations, such as those in VTK, ParaView, and VisIt, implement more
advanced control mechanisms and pass meta-data throughout the pipeline.
These control mechanisms can, for example, subset the data in
space\lcite{Childs05} or time\lcite{Biddiscombe07} based on the needs of
the individual computing units.  Recent work is coupling this mechanism
with query-driven visualization techniques\lcite{Gosink08} to better sift
through large data sets with limited resources.

These control mechanisms can also be used to stream data, in pieces,
through the pipeline\lcite{Ahrens01}.  More recent advances allow us to
prioritize the streaming, thus allowing to compensate for high latency of
streaming by presenting the most relevant data first\lcite{Ahrens07}.  This
in turn has lead to multi-resolution
visualization\lcite{Pascucci01,Woodring09}.  Multiple resolutions further
hide limited resource latency by first presenting low-detailed results and
then iteratively refining them as needed.  This assumes, of course, that a
multi-resolution hierarchy of data is already built (a non-trivial task for
unstructured data).  This work is being implemented in a traditional
visualization pipeline, but could be leveraged in many other types of
frameworks, including the one proposed for this project.

Another recent research project extends the visualization-pipeline
streaming mechanism by automatically orchestrating task concurrency in
independent components of the pipeline\lcite{Vo09}.  The technique adapts
the visualization pipeline to multi-core processors, but it has its
limitations.  There is a high overhead with regard to each execution thread
created; they require isolated buffers of memory for input and output and
independent call stacks, which typically run many calls deep.  Furthermore,
algorithms in the filters are optimized to iterate over sizable data
chunks, which will not be the case with massive multi-threading.  At some
point the algorithms will have to be reengineered to process small data
chunks or themselves be multithreaded.  It will be necessary to leverage a
threading paradigm like the one proposed for this project to engineer this
kind of change on a full-featured toolkit.

An alternative data analysis and visualization architecture is implemented
by the Field Encapsulation Library (FEL)\lcite{FELPaper}.  FEL provides
abstractions that allows programs to access the structure and fields of a
mesh independently from the data storage.  More importantly, FEL uses C++
template constructs to build functional definitions of fields.  These
fields compute values on demand when requested.  These functional fields
are very similar in nature to the functors defined in this proposal.

Although the main concerns addressed by of FEL, mesh flexibility and memory
overhead, is complementary to this project, FEL does not adequately manage
the complexity of massive multi-threading.  To support pervasive parallelism
we need to hide the complexity of work distribution.  Also, as the name
implies, the Field Encapsulation Library is primarily concerned with
defining, accessing, and operating on fields.  There is no mechanism for
topological operations that change or create meshes.  Nor is there any
explicit method for aggregation.  In order to address the varied data
analysis and visualization needs, this project enables these features.

Another system with constructs similar to the framework proposed for this
project is provided by Intel Threading Building Blocks (TBB)\lcite{TBB}, a
popular open-source C++ library for multi-core programming.  In addition to
high-level parallel constructs such as parallel looping and reduction
operations, TBB also provides a simple pipeline execution environment.
Like the system proposed for this project, TBB's pipeline mechanism
partitions data based on available hardware threads, handing-off the
resulting partitions to caller-supplied functors that each iterate over
their assigned ranges to perform computation.  Thus, TBB provides a hybrid
abstraction where callers are isolated from some of the complexity of
scheduling work across multiple cores, but each functor is still
responsible for iteration over its subset of the data.

This approach is appropriate for current architectures where an individual
host has a relatively small number of hardware threads, but requires that
functor authors continue to deal with data organization and iteration
issues on an ad-hoc basis.  Further, TBB does not address issues of
scheduling or communication across multiple hosts in a distributed
platform.  This proposal envisions a stricter separation of
responsibilities where functors are responsible for computation only,
leaving data retrieval and inter-processor communication to separate
iterator components.

The MapReduce programming model\lcite{MapReduce} is also similar in spirit
to our proposed framework.  Like our approach, MapReduce simplifies
parallel programming by defining algorithms in terms of local, stateless
operations.  However MapReduce, not being designed as such, does not have
all the conventions necessary for a fully featured visualization and data
analysis library.  For example, expressing local topological connections
(e.g. cells connected to vertices, vertices connected to cells, or cells
connected to cells) are difficult to express.  The combining of predefined
computation units is not directly supported, nor is the specification of
topological, spatial, or temporal domains.  In contrast, our proposed
system provides the primitives with which visualization and data analysis
programmers are accustomed.

\section{Motivation}
\label{sec:Motivation}


As the scale of supercomputers has progressed from the teraflop to the
petaflop we have enjoyed a resiliency of the message passing model
(embodied in the use of MPI) as an effective means of attaining
scalability.  However, as we consider the high performance computer of the
future, the exascale machine, we discover that this concurrency model will
no longer be sufficient.  All industry trends infer that the exascale
machine will be built using many-core processors containing hundreds to
thousands of cores per chip.  This change in processor design has dramatic
effects on the design of large-scale parallel programs.  As stated by a
recent study by the DARPA Information Processing Techniques
Office\lcite{DARPAExascaleStudy}:

\begin{quote}
  The concurrency challenge is manifest in the need for software to expose
  at least 1000$\times$ more concurrency in applications for Extreme Scale
  systems, relative to current systems. It is further exacerbated by the
  projected memory-computation imbalances in Extreme Scale systems, with
  Bytes/Ops ratios that may drop to values as low as $10^{-2}$ where Bytes
  and Ops represent the main memory and computation capacities of the
  system respectively. These ratios will result in 100$\times$ reductions
  in memory per core relative to Petascale systems, with accompanying
  reductions in memory bandwidth per core.  Thus, a significant fraction of
  software concurrency in Extreme Scale systems must come from exploiting
  more parallelism within the computation performed on a single datum.
\end{quote}

Put simply, efficient concurrency on exascale machines requires a massive
amount of concurrent threads, each performing many operations on a small
and localized piece of data.

Other studies concur.  The Workshop on Visual Analysis and Data Exploration
at Extreme Scale\lcite{VisAnalysisExtremeScale} corroborates the need for
``pervasive parallelism'' throughout visual analysis tools and that data
access is a prime consideration for future tools.  The International
Exascale Software Project's recent road map\lcite{ExascaleRoadMap} also
states a required thousand fold increase in concurrency and that
applications may require ten billion threads.  The road map also notes a
change in I/O and Memory that will ``affect programming models and
optimization.''  Careful consideration of memory access is also expected
to have a dramatic effect on energy consumption as much of the power of an
exascale system will be expended moving data.

Will visualization systems need to run on these exascale systems?  They
undoubtedly will.  Although it has been a common practice to use specialty
high performance platforms for visualization and graphics~\lcite{Wylie01},
this trend is coming to an end.  The cost of creating specialty
visualization computers that are capable of analyzing data generated from
large supercomputer runs is becoming prohibitive\lcite{Childs07}.
Consequently, researchers are beginning to leverage the same supercomputers
used for creating the data\lcite{Peterka09:SC,Peterka09:SciDAC,Yu08}.
This, coupled with a renewed interest in running visualization
\emph{in-situ} with simulations to overcome file I/O
constraints\lcite{SNL092014,Tu06,Ross08}, ensures that high performance
visualization code will run on the same technology as the simulation code
for the foreseeable future.

Visualization pipelines fit poorly into this massive concurrency model; the
granularity of the pipeline computational unit, the filter, is too large.
Each filter must ingest, process, and produce an entire data set when
invoked.  Large scale concurrency today is achieved by replicating the
pipeline and partitioning the input data amongst processes\lcite{Ahrens00}.
However, extreme scale computers would require the data to be broken into
billions of partitions.  The overhead of capturing the connectivity
information between these partitions as well as the overhead of executing
these large computation units on such small partitions of data is too great
to make such an approach practical.

To understand why, consider the sobering comparison between the
Jaguar XT5 partition, a current petascale machine, and the projections for
an exascale machine of by the International Exascale Software Project
RoadMap\lcite{ExascaleRoadMap} given in Table~\ref{table:PetaExaCompare}.
Because processor clock rates are not increasing, an exascale computer
requires a thousand-fold increase in the number of cores.  Furthermore,
trends in processor design suggest that these cores must be hyper-threaded
in order to keep them executing at full efficiency.  In all, to drive a
complete exascale machine will require between one and ten billion
concurrently running threads.

\begin{table}[htbp]
  \centering
  \caption{Comparison of characteristics between petascale and projected
    exascale machines.}
  \label{table:PetaExaCompare}
  \begin{tabular}{llll}
    & Jaguar -- XT5 & Exascale & Increase \\
    \hline
    Cores & 224,256 & \parbox{.7in}{100 million\\ \hspace*{6pt} -- 1 billion} & $\sim{}1,000\times$ \\
    Threads & 224,256 way & 1 -- 10 billion way & $\sim{}50,000\times$ \\
    Memory & 300 Terabytes & 128 Petabytes & $\sim{}500\times$
  \end{tabular}
\end{table}

Most of our current tools rely on MPI for concurrency.  An MPI process has
the overhead of a running program with its own memory space.  A common
process has an overhead of about twenty megabytes.  Running on the entirety
of Jaguar yields an overhead of about 4 terabytes, less than two percent of
the overall available memory.  In contrast, the overhead for using MPI
processes for all the concurrency on an exascale machine requires up to 200
petabytes, possibly exceeding the total memory on the system in overhead
alone.

Even getting around problems with the overhead of MPI, the visualization
pipeline still has inherent problems at this level of concurrency.
Consider using Jaguar to process a one trillion cell mesh.  If we partition
these cells evenly amongst all the cores where replicated pipelines will
process each partition, that yields roughly 5 million cells per pipeline.
General rules of thumb indicate this ratio is optimal for structured grids
when running parallel VTK pipelines\sticky{Cite tutorial}.  Scaling to an
exascale machine, we can project to processing 500 trillion cells
(considering this is the expected growth in system memory).  If we
partition these cells evenly amongst all the necessary cores where
replicated pipelines will process each partition, that yields as few as 50
thousand cells per pipeline.  Here we are starving our pipeline.

Even if we somehow avoid the problem of running on the largest exascale
machines, the problem of a fundamental change in processor architecture
persists.  The parallel visualization pipeline simply does not conform well
to multi-core processors and many-core accelerators.  In response several
researchers are pursing the idea of a \keyterm{hybrid parallel
  pipeline}\sticky{cite algorithms, hank's volume rendering, streams...}.
The hybrid parallel pipeline breaks the problem into two hierarchical
levels.  The first level partitions the data amongst distributed memory
nodes in the same way as the current parallel pipeline.  In the second
level we run a threaded shared memory algorithm to take advantage of a
multi- or many-core processor.

Although the current visualization pipeline does a good job in providing
this first level of distributed memory concurrency, it provides no
facilities whatsoever for this second layer of multi-threaded concurrency.
This places the onus on each visualization pipeline filter developer.  That
is, each filter must be independently and painstakingly designed to exploit
concurrency and optimized for whatever architecture is used.  Even if this
undertaking were to be performed, the concurrency is ultimately undermined
at the connections of filters where execution threads must be synchronized
and data combined.

Our Dax toolkit is designed to encapsulate the complexity of multi-threaded
visualization and data analysis algorithms.  Our initial implementation
targets GPU architectures.  We feel that the idiosyncrasies of these
accelerators, many threads with explicit memory locality, are
representative of all future architectures.

%% VGTC-specific section command.
\acknowledgements{ASCR Attribution.

  Part of this work was performed by Sandia National Laboratory.
  Sandia National Laboratories is a multi-program laboratory operated by
  Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
  Corporation, for the U.S. Department of Energy's National Nuclear
  Security Administration.}

\bibliographystyle{abbrv}
\bibliography{DaxLDAV2011}

\end{document}
